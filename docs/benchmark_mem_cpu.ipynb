{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Size and Speed Benchmarking\n",
    "\n",
    "`Ensembles` are specifically designed for optimal usability, memory usage, and computational speed. In this tutorial we explore the size and speed related characteristics of `Ensembles` compared to using the equivalent individual models. We aim to begin to answer the following questions: \n",
    "- How much memory does an ensemble use when working with it compared to working with the equivalent individual models?\n",
    "- How much disk space is used to store ensembles compared to the equivalent individual models?\n",
    "- How long does it take to run FBA for all members of an ensemble compared to the equivalent individual models?\n",
    "\n",
    "## Ensemble memory requirements during use and when saved\n",
    "\n",
    "`Ensembles` are structured to minimize the amount of memory required when loaded and when being saved. One of the major challenges when working with ensembles of models is having all of the models readily available in memory while conducting analyses. With efficient packaging of the features that are different between members of an ensemble, we were able to significantly reduce the amount of memory and hard drive space required for working with ensembles of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "import medusa\n",
    "import numpy\n",
    "from medusa.test import create_test_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.82 MB\n"
     ]
    }
   ],
   "source": [
    "# RAM required to load in a 1000 member ensemble\n",
    "\n",
    "# Check initial RAM usage\n",
    "RAM_before = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "\n",
    "# Load in test ensemble from file\n",
    "ensemble = create_test_ensemble(\"Staphylococcus aureus\")\n",
    "\n",
    "# Check RAM usage after loading in ensemble\n",
    "RAM_after = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "RAM_used = RAM_after - RAM_before\n",
    "# Print RAM usage increase due to loading ensemble\n",
    "print(\"%.2f\" % (RAM_used), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Members\n"
     ]
    }
   ],
   "source": [
    "# The test S. aureus model has 1000 members\n",
    "print(len(ensemble.members),'Members')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.50 MB\n"
     ]
    }
   ],
   "source": [
    "# RAM required to load a single individual model\n",
    "\n",
    "from copy import deepcopy\n",
    "# Check initial RAM usage\n",
    "RAM_before = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "\n",
    "# Deepcopy base model to create new instance of model in RAM\n",
    "extracted_base_model_copy = deepcopy(ensemble.base_model)\n",
    "\n",
    "# Check RAM usage after loading in ensemble\n",
    "RAM_after = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "RAM_used = RAM_after - RAM_before\n",
    "# Print RAM usage increase due to loading ensemble\n",
    "print(\"%.2f\" % (RAM_used), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500.00 MB or\n",
      "17.09 GB\n"
     ]
    }
   ],
   "source": [
    "# If we were to load the individual base model as 1000 unique\n",
    "# model variables we would use 1000x as much RAM:\n",
    "RAM_used_for_1000_individual_model_variables = RAM_used * 1000\n",
    "print(\"%.2f\" % (RAM_used_for_1000_individual_model_variables), 'MB or')\n",
    "print(\"%.2f\" % (RAM_used_for_1000_individual_model_variables/1024.0), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the ensemble and extracted base model\n",
    "import pickle\n",
    "path = \"../medusa/test/data/benchmarking/\"\n",
    "pickle.dump(ensemble, open(path+\"Staphylococcus_aureus_ensemble1000.pickle\",\"wb\"))\n",
    "pickle.dump(extracted_base_model_copy, open(path+\"Staphylococcus_aureus_base_model.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.67 MB for a 1000 member ensemble\n"
     ]
    }
   ],
   "source": [
    "# Check for file size of ensemble\n",
    "file_path = \"../medusa/test/data/benchmarking/Staphylococcus_aureus_ensemble1000.pickle\"\n",
    "if os.path.isfile(file_path):\n",
    "    file_info = os.stat(file_path)\n",
    "    mb = file_info.st_size/(1024.0**2) # Convert from bytes to MB\n",
    "    print(\"%.2f %s\" % (mb, 'MB for a 1000 member ensemble'))\n",
    "else:\n",
    "    print(\"File path doesn't point to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 MB per model\n",
      "1070.01 MB for 1000 individual model files.\n",
      "1.04 GB for 1000 individual model files.\n"
     ]
    }
   ],
   "source": [
    "# Check for file size of extracted base model\n",
    "file_path = \"../medusa/test/data/benchmarking/Staphylococcus_aureus_base_model.pickle\"\n",
    "if os.path.isfile(file_path):\n",
    "    file_info = os.stat(file_path)\n",
    "    mb = file_info.st_size/(1024.0**2) # Convert from bytes to MB\n",
    "    print(\"%.2f %s\" % (mb, 'MB per model'))\n",
    "else:\n",
    "    print(\"File path doesn't point to file.\")\n",
    "\n",
    "print(\"%.2f\" % (mb*1000),'MB for 1000 individual model files.')\n",
    "print(\"%.2f\" % (mb*1000/1024),'GB for 1000 individual model files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux analysis speed testing\n",
    "\n",
    "Running FBA requires a relatively short amount of time for a single model, however when working with ensembles of 1000s of models, the simple optimization problems can add up to significant amounts of time. Here we explore the expected timeframes for FBA with an ensemble and how that compares to using the equivalent number of individual models. It is important to note that during this benchmarking, we assume that the computer being used is capable to loading all individual modelings into the RAM; this may not be the case for many modern laptop computers (e.g., ~16GB spare memory required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from medusa.flux_analysis import flux_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processors: 87.24728102684021 seconds for entire ensemble\n",
      "2 processors: 44.09945402145386 seconds for entire ensemble\n",
      "3 processors: 32.84902577400207 seconds for entire ensemble\n",
      "4 processors: 27.70060839653015 seconds for entire ensemble\n"
     ]
    }
   ],
   "source": [
    "# Time required to run FBA on a 1000 member ensemble using the innate Medusa functions.\n",
    "runtimes = {}\n",
    "trials = 5\n",
    "for num_processes in [1,2,3,4]:\n",
    "    runtimes[num_processes] = []\n",
    "    for trial in range(0,trials):\n",
    "        t0 = time.time()\n",
    "        flux_balance.optimize_ensemble(ensemble, num_processes = num_processes)\n",
    "        t1 = time.time()\n",
    "        runtimes[num_processes].append(t1-t0)\n",
    "    print(str(num_processes) + ' processors: ' + str(numpy.mean(runtimes[num_processes])) + ' seconds for entire ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.06 seconds for 1000 models\n",
      "34.51 seconds for 1000 models\n",
      "34.49 seconds for 1000 models\n",
      "34.62 seconds for 1000 models\n",
      "34.37 seconds for 1000 models\n",
      "34.61 second average for 1000 models\n"
     ]
    }
   ],
   "source": [
    "# Time required to run FBA on 1000 individual models using a single processor.\n",
    "# This is the equivalent time that would be required if all 1000 models were pre-loaded in RAM.\n",
    "\n",
    "trial_total = []\n",
    "for trial in range(0,trials):\n",
    "    t_total = 0\n",
    "    for member in ensemble.members:\n",
    "        # Set the member state\n",
    "        ensemble.set_state(member.id)\n",
    "        # Start the timer to capture only time required to run FBA on each model\n",
    "        t0 = time.time()\n",
    "        solution = ensemble.base_model.optimize()\n",
    "        t1 = time.time()\n",
    "        t_total = t1-t0 + t_total\n",
    "    print(\"%.2f\" % (t_total) ,'seconds for 1000 models')\n",
    "    trial_total.append(t_total)\n",
    "print(\"%.2f\" % (numpy.mean(trial_total)) ,'second average for 1000 models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using individual models stored in memory is faster than an equivalent ensemble with 1-2 processors,  but Medusa is faster with an increasing number of processors. Keep in mind, however, that this comparison doesn't consider the time it takes to load all of the models (\\~200x faster in Medusa for an ensemble this size), make any modifications to the media conditions for an ensemble (one operation in Medusa; 1000 independent operations with individual models), and that using individual models requires far more memory (\\~300x in this case).\n",
    "\n",
    "This comparison also doesn't factor in the time required for the first optimization performed with any COBRApy model. When a model is optimized once, the solver maintains the solution as a starting point for future optimization steps, substantially reducing the time required for future simulations. Medusa intrinsically takes advantage of this by only using one COBRApy model to represent the entire ensemble; the solution is recycled from member to member during ensemble FBA in Medusa. In contrast, the first optimization step for every individual model loaded into memory will be more computationally expensive, as seen by the timing in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.96 seconds for 1000 models\n"
     ]
    }
   ],
   "source": [
    "# Time required to run FBA on 1000 individual models with a complete solver reset \n",
    "# before each optimization problem is solved. \n",
    "\n",
    "# Load fresh version of model with blank solver state\n",
    "fresh_base_model = pickle.load(open(\"../medusa/test/data/benchmarking/Staphylococcus_aureus_base_model.pickle\",\"rb\"))\n",
    "# Determine how long it takes to run FBA on one individual model\n",
    "t0 = time.time()\n",
    "fresh_base_model.optimize()\n",
    "t1 = time.time()\n",
    "t_total = t1-t0\n",
    "# Calculate how long it would take to run FBA on 1000 unique individual models\n",
    "print(\"%.2f\" % (t_total*1000), 'seconds for 1000 models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medusa_devel",
   "language": "python",
   "name": "medusa_devel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
