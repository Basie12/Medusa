{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Size and Speed Benchmarking\n",
    "\n",
    "`Ensembles` are specifically designed for optimal usability, memory usage, and computational speed. In this tutorial we explore the size and speed related characteristics of `Ensembles` compared to using the equivalent individual models. We aim to begin to answer the following questions: \n",
    "- How much RAM does an ensemble use when working with it compared to working with the equivalent individual models?\n",
    "- How much memory is used to store ensembles compared to the equivalent individual models?\n",
    "- How long does it take to run FBA for all members of an ensemble compared to the equivalent individual models?\n",
    "\n",
    "## Ensemble memory requirements during use and when saved\n",
    "\n",
    "`Ensembles` are structured to minimize the amount of RAM required when loaded and when being saved. One of the major challenges when working with ensembles of models is having all of the models readily available in RAM while conducting analyses. With efficient packaging of the features that are different between members of an ensemble, we were able to significantly reduce the amount of RAM and hard drive space required for working with ensembles of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import psutil\n",
    "import medusa\n",
    "from medusa.test import create_test_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.58 MB\n"
     ]
    }
   ],
   "source": [
    "# RAM required to load in a 1000 member ensemble\n",
    "\n",
    "# Check initial RAM usage\n",
    "RAM_before = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "\n",
    "# Load in test ensemble from file\n",
    "ensemble = create_test_ensemble(\"Staphylococcus aureus\")\n",
    "\n",
    "# Check RAM usage after loading in ensemble\n",
    "RAM_after = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "RAM_used = RAM_after - RAM_before\n",
    "# Print RAM usage increase due to loading ensemble\n",
    "print(\"%.2f\" % (RAM_used), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 Members\n"
     ]
    }
   ],
   "source": [
    "# The test S. aureus model has 1000 members\n",
    "print(len(ensemble.members),'Members')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.23 MB\n"
     ]
    }
   ],
   "source": [
    "# RAM required to load a single individual model\n",
    "\n",
    "from copy import deepcopy\n",
    "# Check initial RAM usage\n",
    "RAM_before = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "\n",
    "# Deepcopy base model to create new instance of model in RAM\n",
    "extracted_base_model_copy = deepcopy(ensemble.base_model)\n",
    "\n",
    "# Check RAM usage after loading in ensemble\n",
    "RAM_after = psutil.Process(os.getpid()).memory_info()[0]/1024**2 # Units = MB\n",
    "RAM_used = RAM_after - RAM_before\n",
    "# Print RAM usage increase due to loading ensemble\n",
    "print(\"%.2f\" % (RAM_used), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19230.47 MB or\n",
      "18.78 GB\n"
     ]
    }
   ],
   "source": [
    "# If we were to load the individual base model as 1000 unique\n",
    "# model variables we would use 1000x as much RAM:\n",
    "RAM_used_for_1000_individual_model_variables = RAM_used * 1000\n",
    "print(\"%.2f\" % (RAM_used_for_1000_individual_model_variables), 'MB or')\n",
    "print(\"%.2f\" % (RAM_used_for_1000_individual_model_variables/1024.0), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the ensemble and extracted base model\n",
    "import pickle\n",
    "path = \"../medusa/test/data/benchmarking/\"\n",
    "pickle.dump(ensemble, open(path+\"Staphylococcus_aureus_ensemble1000.pickle\",\"wb\"))\n",
    "pickle.dump(extracted_base_model_copy, open(path+\"Staphylococcus_aureus_base_model.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.61 MB for a 1000 member ensemble\n"
     ]
    }
   ],
   "source": [
    "# Check for file size of ensemble\n",
    "file_path = \"../medusa/test/data/benchmarking/Staphylococcus_aureus_ensemble1000.pickle\"\n",
    "if os.path.isfile(file_path):\n",
    "    file_info = os.stat(file_path)\n",
    "    mb = file_info.st_size/(1024.0**2) # Convert from bytes to MB\n",
    "    print(\"%.2f %s\" % (mb, 'MB for a 1000 member ensemble'))\n",
    "else:\n",
    "    print(\"File path doesn't point to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17 MB per model\n",
      "1171.96 MB for 1000 individual model files.\n",
      "1.14 GB for 1000 individual model files.\n"
     ]
    }
   ],
   "source": [
    "# Check for file size of extracted base model\n",
    "file_path = \"../medusa/test/data/benchmarking/Staphylococcus_aureus_base_model.pickle\"\n",
    "if os.path.isfile(file_path):\n",
    "    file_info = os.stat(file_path)\n",
    "    mb = file_info.st_size/(1024.0**2) # Convert from bytes to MB\n",
    "    print(\"%.2f %s\" % (mb, 'MB per model'))\n",
    "else:\n",
    "    print(\"File path doesn't point to file.\")\n",
    "\n",
    "print(\"%.2f\" % (mb*1000),'MB for 1000 individual model files.')\n",
    "print(\"%.2f\" % (mb*1000/1024),'GB for 1000 individual model files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux analysis speed testing\n",
    "\n",
    "Running FBA requires a relatively short amount of time to for a single model, however when working with ensembles of 1000s of models, the simple optimization problems can add up to significant amounts of time. Here we explore the expected timeframes for an ensemble and how that compares to using the equivalent number of individual models. It is important to note that during this benchmarking, we assume that the computer being used is capable to loading all individual modelings into the RAM, this may not be the case for many laptop computers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from medusa.flux_analysis import flux_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processors: 142.41587114334106 seconds for entire ensemble\n",
      "2 processors: 79.16171908378601 seconds for entire ensemble\n",
      "4 processors: 44.92253303527832 seconds for entire ensemble\n",
      "8 processors: 34.65370845794678 seconds for entire ensemble\n"
     ]
    }
   ],
   "source": [
    "# Time required to run FBA on a 1000 member ensemble using the innate Medusa functions.\n",
    "runtimes = {}\n",
    "for num_processes in [1,2,4,8]:\n",
    "    t0 = time.time()\n",
    "    flux_balance.optimize_ensemble(ensemble, num_processes = num_processes)\n",
    "    t1 = time.time()\n",
    "    runtimes[num_processes] = t1-t0\n",
    "    print(str(num_processes) + ' processors: ' + str(t1-t0) + ' seconds for entire ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.50 seconds for 1000 models\n"
     ]
    }
   ],
   "source": [
    "# Time required to run FBA on 1000 individual models using a single processor.\n",
    "# This is the equivalent time that would be required if all 1000 models were pre-loaded in RAM.\n",
    "\n",
    "t_total = 0\n",
    "for member in ensemble.members:\n",
    "    # Set the member state \n",
    "    ensemble.set_state(member.id)\n",
    "    # Start the timer to capture only time required to run FBA on each model\n",
    "    t0 = time.time()\n",
    "    solution = ensemble.base_model.optimize()\n",
    "    t1 = time.time()\n",
    "    t_total = t1-t0 + t_total\n",
    "print(\"%.2f\" % (t_total) ,'seconds for 1000 models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading individual models is about twice as fast as using Medusa ensembles (ignoring the time it takes to load all of the models), however requires about 300 times as much RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436.84 seconds for 1000 models\n"
     ]
    }
   ],
   "source": [
    "# Time required to run FBA on 1000 individual models with a complete solver reset \n",
    "# before each optimization problem is solved. \n",
    "\n",
    "# Load fresh version of model with blank solver state\n",
    "fresh_base_model = pickle.load(open(\"../medusa/test/data/benchmarking/Staphylococcus_aureus_base_model.pickle\",\"rb\"))\n",
    "# Determine how long it takes to run FBA on one individual model\n",
    "t0 = time.time()\n",
    "fresh_base_model.optimize()\n",
    "t1 = time.time()\n",
    "t_total = t1-t0\n",
    "# Calculate how long it would take to run FBA on 1000 unique individual models\n",
    "print(\"%.2f\" % (t_total*1000), 'seconds for 1000 models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medusa",
   "language": "python",
   "name": "medusa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
